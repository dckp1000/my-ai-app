#
# Apache Spark Configuration for NBA AI App
#
# This file contains default Spark configuration properties.
# You can override these settings using environment variables or
# command-line arguments when submitting Spark jobs.
#
# Usage:
#   spark-submit --properties-file spark_config.conf spark_app.py
#

# ============================================================
# Application Configuration
# ============================================================

# Application name
spark.app.name=NBA-Data-Analysis

# ============================================================
# Driver Configuration
# ============================================================

# Driver memory allocation
spark.driver.memory=2g

# Driver maximum result size
spark.driver.maxResultSize=1g

# ============================================================
# Executor Configuration
# ============================================================

# Executor memory allocation
spark.executor.memory=2g

# Number of cores per executor
spark.executor.cores=2

# Number of executors (for cluster mode)
spark.executor.instances=2

# ============================================================
# Performance Tuning
# ============================================================

# Enable adaptive query execution
spark.sql.adaptive.enabled=true

# Enable adaptive partition coalescing
spark.sql.adaptive.coalescePartitions.enabled=true

# Minimum number of partitions to coalesce
spark.sql.adaptive.coalescePartitions.minPartitionNum=1

# Enable adaptive skew join optimization
spark.sql.adaptive.skewJoin.enabled=true

# Default parallelism
spark.default.parallelism=4

# SQL shuffle partitions (adjust based on data size)
spark.sql.shuffle.partitions=200

# ============================================================
# Memory Management
# ============================================================

# Fraction of heap space for execution and storage
spark.memory.fraction=0.6

# Fraction of storage memory that is immune to eviction
spark.memory.storageFraction=0.5

# ============================================================
# Serialization
# ============================================================

# Use Kryo serialization for better performance
spark.serializer=org.apache.spark.serializer.KryoSerializer

# Register classes for Kryo serialization
# spark.kryo.registrationRequired=false

# ============================================================
# Shuffle Configuration
# ============================================================

# Enable compression for shuffle
spark.shuffle.compress=true

# Enable compression for RDD partitions
spark.rdd.compress=true

# Compression codec
spark.io.compression.codec=snappy

# ============================================================
# Logging Configuration
# ============================================================

# Log level (ALL, DEBUG, INFO, WARN, ERROR, FATAL, OFF)
spark.log.level=INFO

# ============================================================
# Dynamic Allocation (Optional)
# ============================================================

# Enable dynamic allocation
# spark.dynamicAllocation.enabled=true

# Minimum number of executors
# spark.dynamicAllocation.minExecutors=1

# Maximum number of executors
# spark.dynamicAllocation.maxExecutors=10

# Initial number of executors
# spark.dynamicAllocation.initialExecutors=2

# ============================================================
# UI Configuration
# ============================================================

# Spark UI port
spark.ui.port=4040

# Retain completed applications in UI
spark.ui.retainedJobs=100
spark.ui.retainedStages=100

# ============================================================
# Event Log (for Spark History Server)
# ============================================================

# Enable event logging
# spark.eventLog.enabled=true

# Event log directory
# spark.eventLog.dir=/var/log/spark/events

# Compress event logs
# spark.eventLog.compress=true

# ============================================================
# Checkpoint Configuration
# ============================================================

# Checkpoint directory (optional, for streaming apps)
# spark.checkpoint.dir=/tmp/spark-checkpoint

# ============================================================
# Network Configuration
# ============================================================

# Network timeout
spark.network.timeout=120s

# RPC message max size
spark.rpc.message.maxSize=128

# ============================================================
# Notes
# ============================================================
#
# For production deployments:
# 1. Adjust memory settings based on available cluster resources
# 2. Enable dynamic allocation for better resource utilization
# 3. Configure event logging for monitoring and debugging
# 4. Tune shuffle partitions based on data volume
# 5. Consider using external shuffle service for better stability
#
